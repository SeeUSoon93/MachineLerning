{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "483abee3",
   "metadata": {},
   "source": [
    "### 단어 토큰화(Word Tokenization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade1bd29",
   "metadata": {},
   "source": [
    "- 코퍼스(Corpus):분석에 활용하기 위한 자연어 데이터\n",
    "- 토큰화 : 하나의 코퍼스를 여러 개의 토큰(가장 작은 의미의 단위)으로 나누는 과정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c35eb3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 영어 자연어 처리 패키지 : NLTK\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5b38f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\SeeUSoon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# punkt(토큰화 모듈) : 마침표나 약어(Mr, Dr) 고려하여 진행\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bdf53c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Although', 'it', \"'s\", 'not', 'a', 'happily-ever-after', 'ending', ',', 'it', 'is', 'very', 'realistic', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \"Although it's not a happily-ever-after ending, it is very realistic.\"\n",
    "\n",
    "# 단어 토큰화\n",
    "tokenized_words = word_tokenize(text)\n",
    "\n",
    "# 띄어쓰기, 어퍼스트로피(')가 기준이됨.\n",
    "print(tokenized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970729ce",
   "metadata": {},
   "source": [
    "#### 실습\n",
    "- text.py 파일에는 영어 자연어 코퍼스(말뭉치)가 있습니다. 해당 코퍼스를 불러와서 단어 토큰화를 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "941ee641",
   "metadata": {},
   "outputs": [],
   "source": [
    "from text import TEXT as corpus\n",
    "\n",
    "tokenized_words = word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05f65c43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['After',\n",
       " 'reading',\n",
       " 'the',\n",
       " 'comments',\n",
       " 'for',\n",
       " 'this',\n",
       " 'movie',\n",
       " ',',\n",
       " 'I',\n",
       " 'am',\n",
       " 'not',\n",
       " 'sure',\n",
       " 'whether',\n",
       " 'I',\n",
       " 'should',\n",
       " 'be',\n",
       " 'angry',\n",
       " ',',\n",
       " 'sad',\n",
       " 'or',\n",
       " 'sickened',\n",
       " '.',\n",
       " 'Seeing',\n",
       " 'comments',\n",
       " 'typical',\n",
       " 'of',\n",
       " 'people',\n",
       " 'who',\n",
       " 'a',\n",
       " ')',\n",
       " 'know',\n",
       " 'absolutely',\n",
       " 'nothing',\n",
       " 'about',\n",
       " 'the',\n",
       " 'military',\n",
       " 'or',\n",
       " 'b',\n",
       " ')',\n",
       " 'who',\n",
       " 'base',\n",
       " 'everything',\n",
       " 'they',\n",
       " 'think',\n",
       " 'they',\n",
       " 'know',\n",
       " 'on',\n",
       " 'movies',\n",
       " 'like',\n",
       " 'this',\n",
       " 'or',\n",
       " 'on',\n",
       " 'CNN',\n",
       " 'reports',\n",
       " 'about',\n",
       " 'Abu-Gharib',\n",
       " 'makes',\n",
       " 'me',\n",
       " 'wonder',\n",
       " 'about',\n",
       " 'the',\n",
       " 'state',\n",
       " 'of',\n",
       " 'intellectual',\n",
       " 'stimulation',\n",
       " 'in',\n",
       " 'the',\n",
       " 'world',\n",
       " '.',\n",
       " 'At',\n",
       " 'the',\n",
       " 'time',\n",
       " 'I',\n",
       " 'type',\n",
       " 'this',\n",
       " 'the',\n",
       " 'number',\n",
       " 'of',\n",
       " 'people',\n",
       " 'in',\n",
       " 'the',\n",
       " 'US',\n",
       " 'military',\n",
       " ':',\n",
       " '1.4',\n",
       " 'million',\n",
       " 'on',\n",
       " 'Active',\n",
       " 'Duty',\n",
       " 'with',\n",
       " 'another',\n",
       " 'almost',\n",
       " '900,000',\n",
       " 'in',\n",
       " 'the',\n",
       " 'Guard',\n",
       " 'and',\n",
       " 'Reserves',\n",
       " 'for',\n",
       " 'a',\n",
       " 'total',\n",
       " 'of',\n",
       " 'roughly',\n",
       " '2.3',\n",
       " 'million',\n",
       " '.',\n",
       " 'The',\n",
       " 'number',\n",
       " 'of',\n",
       " 'people',\n",
       " 'indicted',\n",
       " 'for',\n",
       " 'abuses',\n",
       " 'at',\n",
       " 'at',\n",
       " 'Abu-Gharib',\n",
       " ':',\n",
       " 'Currently',\n",
       " 'less',\n",
       " 'than',\n",
       " '20',\n",
       " 'That',\n",
       " 'makes',\n",
       " 'the',\n",
       " 'total',\n",
       " 'of',\n",
       " 'people',\n",
       " 'indicted',\n",
       " '.00083',\n",
       " '%',\n",
       " 'of',\n",
       " 'the',\n",
       " 'total',\n",
       " 'military',\n",
       " '.',\n",
       " 'Even',\n",
       " 'if',\n",
       " 'you',\n",
       " 'indict',\n",
       " 'every',\n",
       " 'single',\n",
       " 'military',\n",
       " 'member',\n",
       " 'that',\n",
       " 'ever',\n",
       " 'stepped',\n",
       " 'in',\n",
       " 'to',\n",
       " 'Abu-Gharib',\n",
       " ',',\n",
       " 'you',\n",
       " 'would',\n",
       " 'not',\n",
       " 'come',\n",
       " 'close',\n",
       " 'to',\n",
       " 'making',\n",
       " 'that',\n",
       " 'a',\n",
       " 'whole',\n",
       " 'number',\n",
       " '.',\n",
       " 'The',\n",
       " 'flaws',\n",
       " 'in',\n",
       " 'this',\n",
       " 'movie',\n",
       " 'would',\n",
       " 'take',\n",
       " 'YEARS',\n",
       " 'to',\n",
       " 'cover',\n",
       " '.',\n",
       " 'I',\n",
       " 'understand',\n",
       " 'that',\n",
       " 'it',\n",
       " \"'s\",\n",
       " 'supposed',\n",
       " 'to',\n",
       " 'be',\n",
       " 'sarcastic',\n",
       " ',',\n",
       " 'but',\n",
       " 'in',\n",
       " 'reality',\n",
       " ',',\n",
       " 'the',\n",
       " 'writer',\n",
       " 'and',\n",
       " 'director',\n",
       " 'are',\n",
       " 'trying',\n",
       " 'to',\n",
       " 'make',\n",
       " 'commentary',\n",
       " 'about',\n",
       " 'the',\n",
       " 'state',\n",
       " 'of',\n",
       " 'the',\n",
       " 'military',\n",
       " 'without',\n",
       " 'an',\n",
       " 'enemy',\n",
       " 'to',\n",
       " 'fight',\n",
       " '.',\n",
       " 'In',\n",
       " 'reality',\n",
       " ',',\n",
       " 'the',\n",
       " 'US',\n",
       " 'military',\n",
       " 'has',\n",
       " 'been',\n",
       " 'at',\n",
       " 'its',\n",
       " 'busiest',\n",
       " 'when',\n",
       " 'there',\n",
       " 'are',\n",
       " 'not',\n",
       " 'conflicts',\n",
       " 'going',\n",
       " 'on',\n",
       " '.',\n",
       " 'The',\n",
       " 'military',\n",
       " 'is',\n",
       " 'the',\n",
       " 'first',\n",
       " 'called',\n",
       " 'for',\n",
       " 'disaster',\n",
       " 'relief',\n",
       " 'and',\n",
       " 'humanitarian',\n",
       " 'aid',\n",
       " 'missions',\n",
       " '.',\n",
       " 'When',\n",
       " 'the',\n",
       " 'tsunami',\n",
       " 'hit',\n",
       " 'Indonesia',\n",
       " ',',\n",
       " 'devestating',\n",
       " 'the',\n",
       " 'region',\n",
       " ',',\n",
       " 'the',\n",
       " 'US',\n",
       " 'military',\n",
       " 'was',\n",
       " 'the',\n",
       " 'first',\n",
       " 'on',\n",
       " 'the',\n",
       " 'scene',\n",
       " '.',\n",
       " 'When',\n",
       " 'the',\n",
       " 'chaos',\n",
       " 'of',\n",
       " 'the',\n",
       " 'situation',\n",
       " 'overwhelmed',\n",
       " 'the',\n",
       " 'local',\n",
       " 'governments',\n",
       " ',',\n",
       " 'it',\n",
       " 'was',\n",
       " 'military',\n",
       " 'leadership',\n",
       " 'who',\n",
       " 'looked',\n",
       " 'at',\n",
       " 'their',\n",
       " 'people',\n",
       " ',',\n",
       " 'the',\n",
       " 'same',\n",
       " 'people',\n",
       " 'this',\n",
       " 'movie',\n",
       " 'mocks',\n",
       " ',',\n",
       " 'and',\n",
       " 'said',\n",
       " 'make',\n",
       " 'it',\n",
       " 'happen',\n",
       " '.',\n",
       " 'Within',\n",
       " 'hours',\n",
       " ',',\n",
       " 'food',\n",
       " 'aid',\n",
       " 'was',\n",
       " 'reaching',\n",
       " 'isolated',\n",
       " 'villages',\n",
       " '.',\n",
       " 'Within',\n",
       " 'days',\n",
       " ',',\n",
       " 'airfields',\n",
       " 'were',\n",
       " 'built',\n",
       " ',',\n",
       " 'cargo',\n",
       " 'aircraft',\n",
       " 'started',\n",
       " 'landing',\n",
       " 'and',\n",
       " 'a',\n",
       " 'food',\n",
       " 'distribution',\n",
       " 'system',\n",
       " 'was',\n",
       " 'up',\n",
       " 'and',\n",
       " 'running',\n",
       " '.',\n",
       " 'Hours',\n",
       " 'and',\n",
       " 'days',\n",
       " ',',\n",
       " 'not',\n",
       " 'weeks',\n",
       " 'and',\n",
       " 'months',\n",
       " '.',\n",
       " 'Yes',\n",
       " 'there',\n",
       " 'are',\n",
       " 'unscrupulous',\n",
       " 'people',\n",
       " 'in',\n",
       " 'the',\n",
       " 'US',\n",
       " 'military',\n",
       " '.',\n",
       " 'But',\n",
       " 'then',\n",
       " ',',\n",
       " 'there',\n",
       " 'are',\n",
       " 'in',\n",
       " 'every',\n",
       " 'walk',\n",
       " 'of',\n",
       " 'life',\n",
       " ',',\n",
       " 'every',\n",
       " 'occupation',\n",
       " '.',\n",
       " 'But',\n",
       " 'to',\n",
       " 'see',\n",
       " 'people',\n",
       " 'on',\n",
       " 'this',\n",
       " 'website',\n",
       " 'decide',\n",
       " 'that',\n",
       " '2.3',\n",
       " 'million',\n",
       " 'men',\n",
       " 'and',\n",
       " 'women',\n",
       " 'are',\n",
       " 'all',\n",
       " 'criminal',\n",
       " ',',\n",
       " 'with',\n",
       " 'nothing',\n",
       " 'on',\n",
       " 'their',\n",
       " 'minds',\n",
       " 'but',\n",
       " 'thoughts',\n",
       " 'of',\n",
       " 'destruction',\n",
       " 'or',\n",
       " 'mayhem',\n",
       " 'is',\n",
       " 'an',\n",
       " 'absolute',\n",
       " 'disservice',\n",
       " 'to',\n",
       " 'the',\n",
       " 'things',\n",
       " 'that',\n",
       " 'they',\n",
       " 'do',\n",
       " 'every',\n",
       " 'day',\n",
       " '.',\n",
       " 'One',\n",
       " 'person',\n",
       " 'on',\n",
       " 'this',\n",
       " 'website',\n",
       " 'even',\n",
       " 'went',\n",
       " 'so',\n",
       " 'far',\n",
       " 'as',\n",
       " 'to',\n",
       " 'say',\n",
       " 'that',\n",
       " 'military',\n",
       " 'members',\n",
       " 'are',\n",
       " 'in',\n",
       " 'it',\n",
       " 'for',\n",
       " 'personal',\n",
       " 'gain',\n",
       " '.',\n",
       " 'Wow',\n",
       " '!',\n",
       " 'Entry',\n",
       " 'level',\n",
       " 'personnel',\n",
       " 'make',\n",
       " 'just',\n",
       " 'under',\n",
       " '$',\n",
       " '8.00',\n",
       " 'an',\n",
       " 'hour',\n",
       " 'assuming',\n",
       " 'a',\n",
       " '40',\n",
       " 'hour',\n",
       " 'work',\n",
       " 'week',\n",
       " '.',\n",
       " 'Of',\n",
       " 'course',\n",
       " ',',\n",
       " 'many',\n",
       " 'work',\n",
       " 'much',\n",
       " 'more',\n",
       " 'than',\n",
       " '40',\n",
       " 'hours',\n",
       " 'a',\n",
       " 'week',\n",
       " 'and',\n",
       " 'those',\n",
       " 'in',\n",
       " 'harm',\n",
       " \"'s\",\n",
       " 'way',\n",
       " 'typically',\n",
       " 'put',\n",
       " 'in',\n",
       " '16-18',\n",
       " 'hour',\n",
       " 'days',\n",
       " 'for',\n",
       " 'months',\n",
       " 'on',\n",
       " 'end',\n",
       " '.',\n",
       " 'That',\n",
       " 'makes',\n",
       " 'the',\n",
       " 'pay',\n",
       " 'well',\n",
       " 'under',\n",
       " 'minimum',\n",
       " 'wage',\n",
       " '.',\n",
       " 'So',\n",
       " 'much',\n",
       " 'for',\n",
       " 'personal',\n",
       " 'gain',\n",
       " '.',\n",
       " 'I',\n",
       " 'beg',\n",
       " 'you',\n",
       " ',',\n",
       " 'please',\n",
       " 'make',\n",
       " 'yourself',\n",
       " 'familiar',\n",
       " 'with',\n",
       " 'the',\n",
       " 'world',\n",
       " 'around',\n",
       " 'you',\n",
       " '.',\n",
       " 'Go',\n",
       " 'to',\n",
       " 'a',\n",
       " 'nearby',\n",
       " 'base',\n",
       " ',',\n",
       " 'get',\n",
       " 'a',\n",
       " 'visitor',\n",
       " 'pass',\n",
       " 'and',\n",
       " 'meet',\n",
       " 'some',\n",
       " 'of',\n",
       " 'the',\n",
       " 'men',\n",
       " 'and',\n",
       " 'women',\n",
       " 'you',\n",
       " 'are',\n",
       " 'so',\n",
       " 'quick',\n",
       " 'to',\n",
       " 'disparage',\n",
       " '.',\n",
       " 'You',\n",
       " 'would',\n",
       " 'be',\n",
       " 'surprised',\n",
       " '.',\n",
       " 'The',\n",
       " 'military',\n",
       " 'no',\n",
       " 'longer',\n",
       " 'accepts',\n",
       " 'people',\n",
       " 'in',\n",
       " 'lieu',\n",
       " 'of',\n",
       " 'prison',\n",
       " 'time',\n",
       " '.',\n",
       " 'They',\n",
       " 'require',\n",
       " 'a',\n",
       " 'minimum',\n",
       " 'of',\n",
       " 'a',\n",
       " 'GED',\n",
       " 'and',\n",
       " 'prefer',\n",
       " 'a',\n",
       " 'high',\n",
       " 'school',\n",
       " 'diploma',\n",
       " '.',\n",
       " 'The',\n",
       " 'middle',\n",
       " 'ranks',\n",
       " 'are',\n",
       " 'expected',\n",
       " 'to',\n",
       " 'get',\n",
       " 'a',\n",
       " 'minimum',\n",
       " 'of',\n",
       " 'undergraduate',\n",
       " 'degrees',\n",
       " 'and',\n",
       " 'the',\n",
       " 'upper',\n",
       " 'ranks',\n",
       " 'are',\n",
       " 'encouraged',\n",
       " 'to',\n",
       " 'get',\n",
       " 'advanced',\n",
       " 'degrees',\n",
       " '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0d8f88",
   "metadata": {},
   "source": [
    "#### 정제\n",
    "- 코퍼스에는 아무 의미도 없거나 목적에 부합하지 않는 단어들도 포함\n",
    "- 전처리 과정에서 토콘들을 제거하는 작업을 **정제**\n",
    "- 등장빈도, 단어길이, 불용어 등을 기준으로 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29612ebd",
   "metadata": {},
   "source": [
    "등장빈도가 적은 단어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4506a9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 등장빈도가 2이하인 단어들만 추출\n",
    "\n",
    "# 단어의 빈도 계산에 사용할 Counter() 함수 사용\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a01fac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 단어 토큰 리스트 = tokenized_words\n",
    "# Counter 함수를 통해 단어의 빈도수를 카운트하여 집합 생성\n",
    "vocab = Counter(tokenized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6219c18b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'After': 1,\n",
       "         'reading': 1,\n",
       "         'the': 30,\n",
       "         'comments': 2,\n",
       "         'for': 7,\n",
       "         'this': 7,\n",
       "         'movie': 3,\n",
       "         ',': 21,\n",
       "         'I': 5,\n",
       "         'am': 1,\n",
       "         'not': 4,\n",
       "         'sure': 1,\n",
       "         'whether': 1,\n",
       "         'should': 1,\n",
       "         'be': 3,\n",
       "         'angry': 1,\n",
       "         'sad': 1,\n",
       "         'or': 4,\n",
       "         'sickened': 1,\n",
       "         '.': 28,\n",
       "         'Seeing': 1,\n",
       "         'typical': 1,\n",
       "         'of': 15,\n",
       "         'people': 9,\n",
       "         'who': 3,\n",
       "         'a': 12,\n",
       "         ')': 2,\n",
       "         'know': 2,\n",
       "         'absolutely': 1,\n",
       "         'nothing': 2,\n",
       "         'about': 4,\n",
       "         'military': 12,\n",
       "         'b': 1,\n",
       "         'base': 2,\n",
       "         'everything': 1,\n",
       "         'they': 3,\n",
       "         'think': 1,\n",
       "         'on': 9,\n",
       "         'movies': 1,\n",
       "         'like': 1,\n",
       "         'CNN': 1,\n",
       "         'reports': 1,\n",
       "         'Abu-Gharib': 3,\n",
       "         'makes': 3,\n",
       "         'me': 1,\n",
       "         'wonder': 1,\n",
       "         'state': 2,\n",
       "         'intellectual': 1,\n",
       "         'stimulation': 1,\n",
       "         'in': 12,\n",
       "         'world': 2,\n",
       "         'At': 1,\n",
       "         'time': 2,\n",
       "         'type': 1,\n",
       "         'number': 3,\n",
       "         'US': 4,\n",
       "         ':': 2,\n",
       "         '1.4': 1,\n",
       "         'million': 3,\n",
       "         'Active': 1,\n",
       "         'Duty': 1,\n",
       "         'with': 3,\n",
       "         'another': 1,\n",
       "         'almost': 1,\n",
       "         '900,000': 1,\n",
       "         'Guard': 1,\n",
       "         'and': 14,\n",
       "         'Reserves': 1,\n",
       "         'total': 3,\n",
       "         'roughly': 1,\n",
       "         '2.3': 2,\n",
       "         'The': 5,\n",
       "         'indicted': 2,\n",
       "         'abuses': 1,\n",
       "         'at': 4,\n",
       "         'Currently': 1,\n",
       "         'less': 1,\n",
       "         'than': 2,\n",
       "         '20': 1,\n",
       "         'That': 2,\n",
       "         '.00083': 1,\n",
       "         '%': 1,\n",
       "         'Even': 1,\n",
       "         'if': 1,\n",
       "         'you': 5,\n",
       "         'indict': 1,\n",
       "         'every': 4,\n",
       "         'single': 1,\n",
       "         'member': 1,\n",
       "         'that': 6,\n",
       "         'ever': 1,\n",
       "         'stepped': 1,\n",
       "         'to': 13,\n",
       "         'would': 3,\n",
       "         'come': 1,\n",
       "         'close': 1,\n",
       "         'making': 1,\n",
       "         'whole': 1,\n",
       "         'flaws': 1,\n",
       "         'take': 1,\n",
       "         'YEARS': 1,\n",
       "         'cover': 1,\n",
       "         'understand': 1,\n",
       "         'it': 4,\n",
       "         \"'s\": 2,\n",
       "         'supposed': 1,\n",
       "         'sarcastic': 1,\n",
       "         'but': 2,\n",
       "         'reality': 2,\n",
       "         'writer': 1,\n",
       "         'director': 1,\n",
       "         'are': 9,\n",
       "         'trying': 1,\n",
       "         'make': 4,\n",
       "         'commentary': 1,\n",
       "         'without': 1,\n",
       "         'an': 3,\n",
       "         'enemy': 1,\n",
       "         'fight': 1,\n",
       "         'In': 1,\n",
       "         'has': 1,\n",
       "         'been': 1,\n",
       "         'its': 1,\n",
       "         'busiest': 1,\n",
       "         'when': 1,\n",
       "         'there': 3,\n",
       "         'conflicts': 1,\n",
       "         'going': 1,\n",
       "         'is': 2,\n",
       "         'first': 2,\n",
       "         'called': 1,\n",
       "         'disaster': 1,\n",
       "         'relief': 1,\n",
       "         'humanitarian': 1,\n",
       "         'aid': 2,\n",
       "         'missions': 1,\n",
       "         'When': 2,\n",
       "         'tsunami': 1,\n",
       "         'hit': 1,\n",
       "         'Indonesia': 1,\n",
       "         'devestating': 1,\n",
       "         'region': 1,\n",
       "         'was': 4,\n",
       "         'scene': 1,\n",
       "         'chaos': 1,\n",
       "         'situation': 1,\n",
       "         'overwhelmed': 1,\n",
       "         'local': 1,\n",
       "         'governments': 1,\n",
       "         'leadership': 1,\n",
       "         'looked': 1,\n",
       "         'their': 2,\n",
       "         'same': 1,\n",
       "         'mocks': 1,\n",
       "         'said': 1,\n",
       "         'happen': 1,\n",
       "         'Within': 2,\n",
       "         'hours': 2,\n",
       "         'food': 2,\n",
       "         'reaching': 1,\n",
       "         'isolated': 1,\n",
       "         'villages': 1,\n",
       "         'days': 3,\n",
       "         'airfields': 1,\n",
       "         'were': 1,\n",
       "         'built': 1,\n",
       "         'cargo': 1,\n",
       "         'aircraft': 1,\n",
       "         'started': 1,\n",
       "         'landing': 1,\n",
       "         'distribution': 1,\n",
       "         'system': 1,\n",
       "         'up': 1,\n",
       "         'running': 1,\n",
       "         'Hours': 1,\n",
       "         'weeks': 1,\n",
       "         'months': 2,\n",
       "         'Yes': 1,\n",
       "         'unscrupulous': 1,\n",
       "         'But': 2,\n",
       "         'then': 1,\n",
       "         'walk': 1,\n",
       "         'life': 1,\n",
       "         'occupation': 1,\n",
       "         'see': 1,\n",
       "         'website': 2,\n",
       "         'decide': 1,\n",
       "         'men': 2,\n",
       "         'women': 2,\n",
       "         'all': 1,\n",
       "         'criminal': 1,\n",
       "         'minds': 1,\n",
       "         'thoughts': 1,\n",
       "         'destruction': 1,\n",
       "         'mayhem': 1,\n",
       "         'absolute': 1,\n",
       "         'disservice': 1,\n",
       "         'things': 1,\n",
       "         'do': 1,\n",
       "         'day': 1,\n",
       "         'One': 1,\n",
       "         'person': 1,\n",
       "         'even': 1,\n",
       "         'went': 1,\n",
       "         'so': 2,\n",
       "         'far': 1,\n",
       "         'as': 1,\n",
       "         'say': 1,\n",
       "         'members': 1,\n",
       "         'personal': 2,\n",
       "         'gain': 2,\n",
       "         'Wow': 1,\n",
       "         '!': 1,\n",
       "         'Entry': 1,\n",
       "         'level': 1,\n",
       "         'personnel': 1,\n",
       "         'just': 1,\n",
       "         'under': 2,\n",
       "         '$': 1,\n",
       "         '8.00': 1,\n",
       "         'hour': 3,\n",
       "         'assuming': 1,\n",
       "         '40': 2,\n",
       "         'work': 2,\n",
       "         'week': 2,\n",
       "         'Of': 1,\n",
       "         'course': 1,\n",
       "         'many': 1,\n",
       "         'much': 2,\n",
       "         'more': 1,\n",
       "         'those': 1,\n",
       "         'harm': 1,\n",
       "         'way': 1,\n",
       "         'typically': 1,\n",
       "         'put': 1,\n",
       "         '16-18': 1,\n",
       "         'end': 1,\n",
       "         'pay': 1,\n",
       "         'well': 1,\n",
       "         'minimum': 3,\n",
       "         'wage': 1,\n",
       "         'So': 1,\n",
       "         'beg': 1,\n",
       "         'please': 1,\n",
       "         'yourself': 1,\n",
       "         'familiar': 1,\n",
       "         'around': 1,\n",
       "         'Go': 1,\n",
       "         'nearby': 1,\n",
       "         'get': 3,\n",
       "         'visitor': 1,\n",
       "         'pass': 1,\n",
       "         'meet': 1,\n",
       "         'some': 1,\n",
       "         'quick': 1,\n",
       "         'disparage': 1,\n",
       "         'You': 1,\n",
       "         'surprised': 1,\n",
       "         'no': 1,\n",
       "         'longer': 1,\n",
       "         'accepts': 1,\n",
       "         'lieu': 1,\n",
       "         'prison': 1,\n",
       "         'They': 1,\n",
       "         'require': 1,\n",
       "         'GED': 1,\n",
       "         'prefer': 1,\n",
       "         'high': 1,\n",
       "         'school': 1,\n",
       "         'diploma': 1,\n",
       "         'middle': 1,\n",
       "         'ranks': 2,\n",
       "         'expected': 1,\n",
       "         'undergraduate': 1,\n",
       "         'degrees': 2,\n",
       "         'upper': 1,\n",
       "         'encouraged': 1,\n",
       "         'advanced': 1})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72d7bfac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['After',\n",
       " 'reading',\n",
       " 'comments',\n",
       " 'am',\n",
       " 'sure',\n",
       " 'whether',\n",
       " 'should',\n",
       " 'angry',\n",
       " 'sad',\n",
       " 'sickened',\n",
       " 'Seeing',\n",
       " 'typical',\n",
       " ')',\n",
       " 'know',\n",
       " 'absolutely',\n",
       " 'nothing',\n",
       " 'b',\n",
       " 'base',\n",
       " 'everything',\n",
       " 'think',\n",
       " 'movies',\n",
       " 'like',\n",
       " 'CNN',\n",
       " 'reports',\n",
       " 'me',\n",
       " 'wonder',\n",
       " 'state',\n",
       " 'intellectual',\n",
       " 'stimulation',\n",
       " 'world',\n",
       " 'At',\n",
       " 'time',\n",
       " 'type',\n",
       " ':',\n",
       " '1.4',\n",
       " 'Active',\n",
       " 'Duty',\n",
       " 'another',\n",
       " 'almost',\n",
       " '900,000',\n",
       " 'Guard',\n",
       " 'Reserves',\n",
       " 'roughly',\n",
       " '2.3',\n",
       " 'indicted',\n",
       " 'abuses',\n",
       " 'Currently',\n",
       " 'less',\n",
       " 'than',\n",
       " '20',\n",
       " 'That',\n",
       " '.00083',\n",
       " '%',\n",
       " 'Even',\n",
       " 'if',\n",
       " 'indict',\n",
       " 'single',\n",
       " 'member',\n",
       " 'ever',\n",
       " 'stepped',\n",
       " 'come',\n",
       " 'close',\n",
       " 'making',\n",
       " 'whole',\n",
       " 'flaws',\n",
       " 'take',\n",
       " 'YEARS',\n",
       " 'cover',\n",
       " 'understand',\n",
       " \"'s\",\n",
       " 'supposed',\n",
       " 'sarcastic',\n",
       " 'but',\n",
       " 'reality',\n",
       " 'writer',\n",
       " 'director',\n",
       " 'trying',\n",
       " 'commentary',\n",
       " 'without',\n",
       " 'enemy',\n",
       " 'fight',\n",
       " 'In',\n",
       " 'has',\n",
       " 'been',\n",
       " 'its',\n",
       " 'busiest',\n",
       " 'when',\n",
       " 'conflicts',\n",
       " 'going',\n",
       " 'is',\n",
       " 'first',\n",
       " 'called',\n",
       " 'disaster',\n",
       " 'relief',\n",
       " 'humanitarian',\n",
       " 'aid',\n",
       " 'missions',\n",
       " 'When',\n",
       " 'tsunami',\n",
       " 'hit',\n",
       " 'Indonesia',\n",
       " 'devestating',\n",
       " 'region',\n",
       " 'scene',\n",
       " 'chaos',\n",
       " 'situation',\n",
       " 'overwhelmed',\n",
       " 'local',\n",
       " 'governments',\n",
       " 'leadership',\n",
       " 'looked',\n",
       " 'their',\n",
       " 'same',\n",
       " 'mocks',\n",
       " 'said',\n",
       " 'happen',\n",
       " 'Within',\n",
       " 'hours',\n",
       " 'food',\n",
       " 'reaching',\n",
       " 'isolated',\n",
       " 'villages',\n",
       " 'airfields',\n",
       " 'were',\n",
       " 'built',\n",
       " 'cargo',\n",
       " 'aircraft',\n",
       " 'started',\n",
       " 'landing',\n",
       " 'distribution',\n",
       " 'system',\n",
       " 'up',\n",
       " 'running',\n",
       " 'Hours',\n",
       " 'weeks',\n",
       " 'months',\n",
       " 'Yes',\n",
       " 'unscrupulous',\n",
       " 'But',\n",
       " 'then',\n",
       " 'walk',\n",
       " 'life',\n",
       " 'occupation',\n",
       " 'see',\n",
       " 'website',\n",
       " 'decide',\n",
       " 'men',\n",
       " 'women',\n",
       " 'all',\n",
       " 'criminal',\n",
       " 'minds',\n",
       " 'thoughts',\n",
       " 'destruction',\n",
       " 'mayhem',\n",
       " 'absolute',\n",
       " 'disservice',\n",
       " 'things',\n",
       " 'do',\n",
       " 'day',\n",
       " 'One',\n",
       " 'person',\n",
       " 'even',\n",
       " 'went',\n",
       " 'so',\n",
       " 'far',\n",
       " 'as',\n",
       " 'say',\n",
       " 'members',\n",
       " 'personal',\n",
       " 'gain',\n",
       " 'Wow',\n",
       " '!',\n",
       " 'Entry',\n",
       " 'level',\n",
       " 'personnel',\n",
       " 'just',\n",
       " 'under',\n",
       " '$',\n",
       " '8.00',\n",
       " 'assuming',\n",
       " '40',\n",
       " 'work',\n",
       " 'week',\n",
       " 'Of',\n",
       " 'course',\n",
       " 'many',\n",
       " 'much',\n",
       " 'more',\n",
       " 'those',\n",
       " 'harm',\n",
       " 'way',\n",
       " 'typically',\n",
       " 'put',\n",
       " '16-18',\n",
       " 'end',\n",
       " 'pay',\n",
       " 'well',\n",
       " 'wage',\n",
       " 'So',\n",
       " 'beg',\n",
       " 'please',\n",
       " 'yourself',\n",
       " 'familiar',\n",
       " 'around',\n",
       " 'Go',\n",
       " 'nearby',\n",
       " 'visitor',\n",
       " 'pass',\n",
       " 'meet',\n",
       " 'some',\n",
       " 'quick',\n",
       " 'disparage',\n",
       " 'You',\n",
       " 'surprised',\n",
       " 'no',\n",
       " 'longer',\n",
       " 'accepts',\n",
       " 'lieu',\n",
       " 'prison',\n",
       " 'They',\n",
       " 'require',\n",
       " 'GED',\n",
       " 'prefer',\n",
       " 'high',\n",
       " 'school',\n",
       " 'diploma',\n",
       " 'middle',\n",
       " 'ranks',\n",
       " 'expected',\n",
       " 'undergraduate',\n",
       " 'degrees',\n",
       " 'upper',\n",
       " 'encouraged',\n",
       " 'advanced']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 빈도수가 2이하인 단어 리스트를 추출\n",
    "# items() 함수 활용 = 키와 값들의 쌍을 얻을 수 있게 해줌\n",
    "vocab.items()\n",
    "# 리스트 컴프리헨션\n",
    "low_freq_words=[key for key, value in vocab.items() if value <= 2]\n",
    "low_freq_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "387496ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['After',\n",
       " 'reading',\n",
       " 'comments',\n",
       " 'am',\n",
       " 'sure',\n",
       " 'whether',\n",
       " 'should',\n",
       " 'angry',\n",
       " 'sad',\n",
       " 'sickened',\n",
       " 'Seeing',\n",
       " 'typical',\n",
       " ')',\n",
       " 'know',\n",
       " 'absolutely',\n",
       " 'nothing',\n",
       " 'b',\n",
       " 'base',\n",
       " 'everything',\n",
       " 'think',\n",
       " 'movies',\n",
       " 'like',\n",
       " 'CNN',\n",
       " 'reports',\n",
       " 'me',\n",
       " 'wonder',\n",
       " 'state',\n",
       " 'intellectual',\n",
       " 'stimulation',\n",
       " 'world',\n",
       " 'At',\n",
       " 'time',\n",
       " 'type',\n",
       " ':',\n",
       " '1.4',\n",
       " 'Active',\n",
       " 'Duty',\n",
       " 'another',\n",
       " 'almost',\n",
       " '900,000',\n",
       " 'Guard',\n",
       " 'Reserves',\n",
       " 'roughly',\n",
       " '2.3',\n",
       " 'indicted',\n",
       " 'abuses',\n",
       " 'Currently',\n",
       " 'less',\n",
       " 'than',\n",
       " '20',\n",
       " 'That',\n",
       " '.00083',\n",
       " '%',\n",
       " 'Even',\n",
       " 'if',\n",
       " 'indict',\n",
       " 'single',\n",
       " 'member',\n",
       " 'ever',\n",
       " 'stepped',\n",
       " 'come',\n",
       " 'close',\n",
       " 'making',\n",
       " 'whole',\n",
       " 'flaws',\n",
       " 'take',\n",
       " 'YEARS',\n",
       " 'cover',\n",
       " 'understand',\n",
       " \"'s\",\n",
       " 'supposed',\n",
       " 'sarcastic',\n",
       " 'but',\n",
       " 'reality',\n",
       " 'writer',\n",
       " 'director',\n",
       " 'trying',\n",
       " 'commentary',\n",
       " 'without',\n",
       " 'enemy',\n",
       " 'fight',\n",
       " 'In',\n",
       " 'has',\n",
       " 'been',\n",
       " 'its',\n",
       " 'busiest',\n",
       " 'when',\n",
       " 'conflicts',\n",
       " 'going',\n",
       " 'is',\n",
       " 'first',\n",
       " 'called',\n",
       " 'disaster',\n",
       " 'relief',\n",
       " 'humanitarian',\n",
       " 'aid',\n",
       " 'missions',\n",
       " 'When',\n",
       " 'tsunami',\n",
       " 'hit',\n",
       " 'Indonesia',\n",
       " 'devestating',\n",
       " 'region',\n",
       " 'scene',\n",
       " 'chaos',\n",
       " 'situation',\n",
       " 'overwhelmed',\n",
       " 'local',\n",
       " 'governments',\n",
       " 'leadership',\n",
       " 'looked',\n",
       " 'their',\n",
       " 'same',\n",
       " 'mocks',\n",
       " 'said',\n",
       " 'happen',\n",
       " 'Within',\n",
       " 'hours',\n",
       " 'food',\n",
       " 'reaching',\n",
       " 'isolated',\n",
       " 'villages',\n",
       " 'airfields',\n",
       " 'were',\n",
       " 'built',\n",
       " 'cargo',\n",
       " 'aircraft',\n",
       " 'started',\n",
       " 'landing',\n",
       " 'distribution',\n",
       " 'system',\n",
       " 'up',\n",
       " 'running',\n",
       " 'Hours',\n",
       " 'weeks',\n",
       " 'months',\n",
       " 'Yes',\n",
       " 'unscrupulous',\n",
       " 'But',\n",
       " 'then',\n",
       " 'walk',\n",
       " 'life',\n",
       " 'occupation',\n",
       " 'see',\n",
       " 'website',\n",
       " 'decide',\n",
       " 'men',\n",
       " 'women',\n",
       " 'all',\n",
       " 'criminal',\n",
       " 'minds',\n",
       " 'thoughts',\n",
       " 'destruction',\n",
       " 'mayhem',\n",
       " 'absolute',\n",
       " 'disservice',\n",
       " 'things',\n",
       " 'do',\n",
       " 'day',\n",
       " 'One',\n",
       " 'person',\n",
       " 'even',\n",
       " 'went',\n",
       " 'so',\n",
       " 'far',\n",
       " 'as',\n",
       " 'say',\n",
       " 'members',\n",
       " 'personal',\n",
       " 'gain',\n",
       " 'Wow',\n",
       " '!',\n",
       " 'Entry',\n",
       " 'level',\n",
       " 'personnel',\n",
       " 'just',\n",
       " 'under',\n",
       " '$',\n",
       " '8.00',\n",
       " 'assuming',\n",
       " '40',\n",
       " 'work',\n",
       " 'week',\n",
       " 'Of',\n",
       " 'course',\n",
       " 'many',\n",
       " 'much',\n",
       " 'more',\n",
       " 'those',\n",
       " 'harm',\n",
       " 'way',\n",
       " 'typically',\n",
       " 'put',\n",
       " '16-18',\n",
       " 'end',\n",
       " 'pay',\n",
       " 'well',\n",
       " 'wage',\n",
       " 'So',\n",
       " 'beg',\n",
       " 'please',\n",
       " 'yourself',\n",
       " 'familiar',\n",
       " 'around',\n",
       " 'Go',\n",
       " 'nearby',\n",
       " 'visitor',\n",
       " 'pass',\n",
       " 'meet',\n",
       " 'some',\n",
       " 'quick',\n",
       " 'disparage',\n",
       " 'You',\n",
       " 'surprised',\n",
       " 'no',\n",
       " 'longer',\n",
       " 'accepts',\n",
       " 'lieu',\n",
       " 'prison',\n",
       " 'They',\n",
       " 'require',\n",
       " 'GED',\n",
       " 'prefer',\n",
       " 'high',\n",
       " 'school',\n",
       " 'diploma',\n",
       " 'middle',\n",
       " 'ranks',\n",
       " 'expected',\n",
       " 'undergraduate',\n",
       " 'degrees',\n",
       " 'upper',\n",
       " 'encouraged',\n",
       " 'advanced']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 빈도수가 2이하인 단어 리스트를 추출\n",
    "# 반복문을 사용해서 추출\n",
    "low_freq_words2 = []\n",
    "for k,v in vocab.items() :\n",
    "    if v <= 2 :\n",
    "        low_freq_words2.append(k)\n",
    "low_freq_words2        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1c97285",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "low_freq_words == low_freq_words2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3973cff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "등장빈도가 2 이하인 단어 수 :  234\n"
     ]
    }
   ],
   "source": [
    "# 등장빈도가 2 이하인 단어 리스트 개수확인\n",
    "print(\"등장빈도가 2 이하인 단어 수 : \",len(low_freq_words2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e18bec2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'for', 'this', 'movie', ',', 'I', 'not', 'be', 'or', '.', 'of', 'people', 'who', 'a', 'about', 'military', 'they', 'on', 'Abu-Gharib', 'makes', 'in', 'number', 'US', 'million', 'with', 'and', 'total', 'The', 'at', 'you', 'every', 'that', 'to', 'would', 'it', 'are', 'make', 'an', 'there', 'was', 'days', 'hour', 'minimum', 'get']\n",
      "['the', 'for', 'this', 'movie', ',', 'I', 'not', 'be', 'or', '.', 'of', 'people', 'who', 'a', 'about', 'military', 'they', 'on', 'Abu-Gharib', 'makes', 'in', 'number', 'US', 'million', 'with', 'and', 'total', 'The', 'at', 'you', 'every', 'that', 'to', 'would', 'it', 'are', 'make', 'an', 'there', 'was', 'days', 'hour', 'minimum', 'get']\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# 등장빈도가 2 이하인 단어들을 제거한 결과를 따로 저장(cleand_by_freq)\n",
    "# not in, 반복문(for),(조건문if)\n",
    "cleaned_by_freq=[]\n",
    "cleaned_by_freq2=[]\n",
    "for k,v in vocab.items() :\n",
    "    if v > 2 :\n",
    "       cleaned_by_freq.append(k)\n",
    "print(cleaned_by_freq)\n",
    "\n",
    "for k,v in vocab.items() :\n",
    "    if k not in low_freq_words :\n",
    "        cleaned_by_freq2.append(k)\n",
    "print(cleaned_by_freq2)\n",
    "\n",
    "print(cleaned_by_freq==cleaned_by_freq2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22d25177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "등장빈도가 3 이상인 단어 수 :  44\n"
     ]
    }
   ],
   "source": [
    "# 등장빈도가 3 이상인 단어 리스트 개수확인\n",
    "print(\"등장빈도가 3 이상인 단어 수 : \",len(cleaned_by_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5470cc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 길이가 2이하인 단어 제거\n",
    "# 콤마, I, be,..등 의미를 갖지 않는 단어 제거\n",
    "cleaned_by_freq_len = []\n",
    "\n",
    "# cleaned_by_freq에서 길이가 2이하인 단어를 제거\n",
    "# 제거된 값은 cleaned_by_freq_len 에 담기\n",
    "for i in cleaned_by_freq :\n",
    "    if len(i) > 2 :\n",
    "        cleaned_by_freq_len.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "15ce08a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'for', 'this', 'movie', 'not', 'people', 'who', 'about', 'military', 'they', 'Abu-Gharib', 'makes', 'number', 'million', 'with', 'and', 'total', 'The', 'you', 'every', 'that', 'would', 'are', 'make', 'there', 'was', 'days', 'hour', 'minimum', 'get']\n"
     ]
    }
   ],
   "source": [
    "print(cleaned_by_freq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad4c286",
   "metadata": {},
   "source": [
    "### 정제함수 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8fb79927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 등장 빈도 기준 정제 함수 정의\n",
    "def clean_by_freq(tokenized_words, freq):\n",
    "    # 1. Counter 함수를 통해 단어의 빈도수를 카운트하여 단어 집합 생성\n",
    "    # Vocab 이라는 변수에 담기\n",
    "    vocab = Counter(tokenized_words)\n",
    "    \n",
    "    # 2. 빈도수가 freq 이하인 단어 추출\n",
    "    # low_freq_words 라는 변수에 담기\n",
    "    low_freq_words = []\n",
    "    for k,v in vocab.items() :\n",
    "        if v <= freq :\n",
    "            low_freq_words.append(k)\n",
    "            \n",
    "    # 3. low_freq_words 에 포함되지 않는 단어 리스트 생성\n",
    "    # cleaned_words 라는 변수에 담기\n",
    "    cleaned_words=[]\n",
    "    for k,v in vocab.items() :\n",
    "        if k not in low_freq_words :\n",
    "            cleaned_words.append(k)\n",
    "    return cleaned_words\n",
    "\n",
    "# 단어 길이 기준 정제 함수\n",
    "def cleane_by_len(tokenized_words, length):\n",
    "    cleaned_by_freq_len = []\n",
    "    for i in tokenized_words :\n",
    "        if len(i) >= length :\n",
    "            cleaned_by_freq_len.append(i)\n",
    "    return cleaned_by_freq_len"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752cc501",
   "metadata": {},
   "source": [
    "### 불용어(Stopwords)\n",
    "- 코퍼스에서 큰 의미가 없거나 분석 목적에서 벗어나는 단어들\n",
    "\n",
    "1. 불용어를 모아 놓은 불용어 세트 준비\n",
    "2. 코퍼스의 각 단어들이 불용어 세트에 포함되는지 확인\n",
    "3. 불용어 세트에 있는 단어 토큰은 분석에서 제외(삭제)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9e637997",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\SeeUSoon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16d04b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불용어 개수 :  179\n",
      "{'there', 'y', 'me', 'over', 'o', 'in', 'off', 'with', 'weren', 'mustn', \"isn't\", 'shouldn', 'further', 'which', 'itself', 'ain', 'this', \"mustn't\", 'themselves', 'them', 'were', 'because', 'that', 'can', 'very', 'don', 'but', 'you', 'our', 'why', 'not', 'only', 'these', 'above', \"that'll\", 'after', \"hadn't\", 'myself', \"didn't\", 'she', 'such', \"shouldn't\", 'are', 'having', 'an', 'has', \"it's\", 'under', \"she's\", 'then', 'hasn', 'didn', 'again', 'down', \"hasn't\", 'won', 't', 'each', \"don't\", 'm', \"doesn't\", 'am', 'at', 'about', \"won't\", \"you've\", 'on', 'does', 'couldn', 'a', 'those', 'him', 'do', 'any', 'few', 'more', 'of', 'same', 'below', 'mightn', 'before', 'if', 'my', 'and', 'against', 'some', \"you're\", \"you'll\", \"couldn't\", \"mightn't\", 'who', \"wouldn't\", \"haven't\", 'wasn', \"needn't\", 'yourself', 'being', 'through', 'he', 'what', 'as', 'during', 'how', \"should've\", 'yourselves', 'from', 'be', 'than', 'theirs', 'own', 'doing', 'by', 'did', 'his', 'no', \"wasn't\", 'ma', 'for', 'or', 'nor', 'now', 'ours', 're', 'up', \"shan't\", 'herself', 'd', 'once', 'll', 'other', 'will', 'where', 'wouldn', 'her', 'hers', 'is', 'into', 'just', 'had', 'when', 'yours', 've', \"weren't\", 'too', 'aren', 'it', 'its', 'ourselves', \"aren't\", 'their', 'your', 'shan', 'most', 'was', 'so', 'hadn', 'until', 'doesn', 'himself', 'out', 'all', \"you'd\", 'haven', 'have', 'while', 'the', 'both', 'should', 's', 'been', 'isn', 'we', 'whom', 'needn', 'i', 'between', 'here', 'they', 'to'}\n"
     ]
    }
   ],
   "source": [
    "# 불용어 세트 확인\n",
    "# 분석에 크게 활용되지 않는 불용어 세트\n",
    "# 집합 자료형(set) : 중복을 허용하지 않는 자료형 / 순서가 존재X\n",
    "stopwords_set = set(stopwords.words('english'))\n",
    "\n",
    "# 불용어 개수 확인\n",
    "# 179개의 기본 불용어 목록 제공\n",
    "print('불용어 개수 : ', len(stopwords_set))\n",
    "print(stopwords_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "54502935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 목록에 새로운 단어 추가, 제거\n",
    "# add(), remove()\n",
    "stopwords_set.add('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0e73409a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불용어 개수 :  180\n",
      "{'there', 'y', 'me', 'over', 'o', 'in', 'off', 'with', 'weren', 'mustn', \"isn't\", 'shouldn', 'further', 'which', 'itself', 'ain', 'this', \"mustn't\", 'themselves', 'them', 'were', 'hello', 'because', 'that', 'can', 'very', 'don', 'but', 'you', 'our', 'why', 'not', 'only', 'these', 'above', \"that'll\", 'after', \"hadn't\", 'myself', \"didn't\", 'she', 'such', \"shouldn't\", 'are', 'having', 'an', 'has', \"it's\", 'under', \"she's\", 'then', 'hasn', 'didn', 'again', 'down', \"hasn't\", 'won', 't', 'each', \"don't\", 'm', \"doesn't\", 'am', 'at', 'about', \"won't\", \"you've\", 'on', 'does', 'couldn', 'a', 'those', 'him', 'do', 'any', 'few', 'more', 'of', 'same', 'below', 'mightn', 'before', 'if', 'my', 'and', 'against', 'some', \"you're\", \"you'll\", \"couldn't\", \"mightn't\", 'who', \"wouldn't\", \"haven't\", 'wasn', \"needn't\", 'yourself', 'being', 'through', 'he', 'what', 'as', 'during', 'how', \"should've\", 'yourselves', 'from', 'be', 'than', 'theirs', 'own', 'doing', 'by', 'did', 'his', 'no', \"wasn't\", 'ma', 'for', 'or', 'nor', 'now', 'ours', 're', 'up', \"shan't\", 'herself', 'd', 'once', 'll', 'other', 'will', 'where', 'wouldn', 'her', 'hers', 'is', 'into', 'just', 'had', 'when', 'yours', 've', \"weren't\", 'too', 'aren', 'it', 'its', 'ourselves', \"aren't\", 'their', 'your', 'shan', 'most', 'was', 'so', 'hadn', 'until', 'doesn', 'himself', 'out', 'all', \"you'd\", 'haven', 'have', 'while', 'the', 'both', 'should', 's', 'been', 'isn', 'we', 'whom', 'needn', 'i', 'between', 'here', 'they', 'to'}\n"
     ]
    }
   ],
   "source": [
    "print('불용어 개수 : ', len(stopwords_set))\n",
    "print(stopwords_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "14cd1294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불용어 개수 :  179\n",
      "{'there', 'y', 'me', 'over', 'o', 'in', 'off', 'with', 'weren', 'mustn', \"isn't\", 'shouldn', 'further', 'which', 'itself', 'ain', 'this', \"mustn't\", 'themselves', 'them', 'were', 'because', 'that', 'can', 'very', 'don', 'but', 'you', 'our', 'why', 'not', 'only', 'these', 'above', \"that'll\", 'after', \"hadn't\", 'myself', \"didn't\", 'she', 'such', \"shouldn't\", 'are', 'having', 'an', 'has', \"it's\", 'under', \"she's\", 'then', 'hasn', 'didn', 'again', 'down', \"hasn't\", 'won', 't', 'each', \"don't\", 'm', \"doesn't\", 'am', 'at', 'about', \"won't\", \"you've\", 'on', 'does', 'couldn', 'a', 'those', 'him', 'do', 'any', 'few', 'more', 'of', 'same', 'below', 'mightn', 'before', 'if', 'my', 'and', 'against', 'some', \"you're\", \"you'll\", \"couldn't\", \"mightn't\", 'who', \"wouldn't\", \"haven't\", 'wasn', \"needn't\", 'yourself', 'being', 'through', 'he', 'what', 'as', 'during', 'how', \"should've\", 'yourselves', 'from', 'be', 'than', 'theirs', 'own', 'doing', 'by', 'did', 'his', 'no', \"wasn't\", 'ma', 'for', 'or', 'nor', 'now', 'ours', 're', 'up', \"shan't\", 'herself', 'd', 'once', 'll', 'other', 'will', 'where', 'wouldn', 'her', 'hers', 'is', 'into', 'just', 'had', 'when', 'yours', 've', \"weren't\", 'too', 'aren', 'it', 'its', 'ourselves', \"aren't\", 'their', 'your', 'shan', 'most', 'was', 'so', 'hadn', 'until', 'doesn', 'himself', 'out', 'all', \"you'd\", 'haven', 'have', 'while', 'the', 'both', 'should', 's', 'been', 'isn', 'we', 'whom', 'needn', 'i', 'between', 'here', 'they', 'to'}\n"
     ]
    }
   ],
   "source": [
    "stopwords_set.remove('hello')\n",
    "print('불용어 개수 : ', len(stopwords_set))\n",
    "print(stopwords_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "93032edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용자가 직접 불용어 세트를 정의\n",
    "my_stopwords_set = {\"you're\", 'doing', 'each', 's', 'ma', 'she', 'below', 'will', 'now', \"you've\", 'about', 'up', 'me', 'same', 'wasn', 'his', \"you'd\", 'a', \"hadn't\", 'i', \"should've\", 'as', 'with', 'over', 'just', 'do', \"mightn't\", 'those', \"shan't\", 'further', 'y', 'the', 'mightn', 'yourself', 'are', \"you'll\", 'they', 'm', 'wouldn', 'that', \"haven't\", 'our', 'above', 'after'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fecafeb",
   "metadata": {},
   "source": [
    "### 불용어(Stopwords) 제거하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "33602fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['movie', 'people', 'military', 'Abu-Gharib', 'makes', 'number', 'million', 'total', 'every', 'would', 'make', 'days', 'hour', 'minimum', 'get']\n"
     ]
    }
   ],
   "source": [
    "cleaned_word = []\n",
    "\n",
    "for word in cleaned_by_freq_len :\n",
    "    if word.lower() not in stopwords_set :\n",
    "        cleaned_word.append(word)\n",
    "print(cleaned_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323cb812",
   "metadata": {},
   "source": [
    "### 불용어 처리 함수 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2d19c136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 제거 함수 만들기\n",
    "def clean_by_stopwords(tokenized_words, stopwords_set):\n",
    "    cleaned_word = []\n",
    "    for word in tokenized_words :\n",
    "        if word not in stopwords_set :\n",
    "            cleaned_word.append(word)\n",
    "    return cleaned_word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5019131b",
   "metadata": {},
   "source": [
    "### 정규화(Normalization)\n",
    "- The, the -> 통합\n",
    "- USA, America, United Stated America... -> 통합\n",
    "- 형태가 다르지만 같은 의미를 나타내는 단어들이 많을 수록 분석이 어려워짐\n",
    "- 의미가 같은 단어라면 형태를 하나로 통일 : 정규화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c57a918",
   "metadata": {},
   "source": [
    "#### 정규화방법 1.대소문자 통합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5b41aa49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what can i do for you? do your homework now.\n"
     ]
    }
   ],
   "source": [
    "text = \"What can I do for you? Do your homework now.\"\n",
    "\n",
    "# 소문자로 변환\n",
    "print(text.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1014ac15",
   "metadata": {},
   "source": [
    "#### 정규화방법 2. 규칙기반 정규화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3eeb52ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 동의어 사전을 만들어서 정규화 진행\n",
    "text = \"She became a US citizen. Ummmm, I think, maybe and or.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8945032c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['She',\n",
       " 'became',\n",
       " 'a',\n",
       " 'USA',\n",
       " 'citizen',\n",
       " '.',\n",
       " 'Umm',\n",
       " ',',\n",
       " 'I',\n",
       " 'think',\n",
       " ',',\n",
       " 'maybe',\n",
       " 'and',\n",
       " 'or',\n",
       " '.']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synonym_dict = {'US' : 'USA', 'U.S':'USA','Ummmm':'Umm','ummm':'umm'}\n",
    "\n",
    "# 단어 토큰화\n",
    "tokenize_words= word_tokenize(text)\n",
    "normalized_words=[]\n",
    "# 동의어 사전에 있는 단어라면, value에 해당하는 값으로 변환\n",
    "for word in tokenize_words :\n",
    "    if word in synonym_dict.keys():\n",
    "        word = synonym_dict[word]        \n",
    "    normalized_words.append(word)\n",
    "normalized_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0358a405",
   "metadata": {},
   "source": [
    "### 어간추출\n",
    "- 어간(Stem) : 특정한 단어의 핵심이 ㅗ디는 부분\n",
    "- 어간추출(Stemming) : 단어에서 어간을 찾아내는 것\n",
    "\n",
    "- Formalize -> Formal(alize -> all)\n",
    "- Relational -> Reaplate(ational -> ate)\n",
    "- Activate -> Activ(ate - 제거)\n",
    "\n",
    "    - 코퍼스의 특성이나 상황에 따라 어간 추출을 하는게 적합할지를 판단\n",
    "    - 분석에 활용해야하는 중요한 단어가 손실될 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f6e805",
   "metadata": {},
   "source": [
    "#### NLTK로 어간 추출하기\n",
    "- 어간 추출을 위한 알고리즘\n",
    "    - 포터 스테머(Poter Stemmer)\n",
    "    - 랭커스터 스테머(Lancaster Stemmer)\n",
    "    - 어간을 추출하는 기준이 미세하게 다름"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c244cbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aa9209de",
   "metadata": {},
   "outputs": [],
   "source": [
    "porter_stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "85abb3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"You are so lovely. I am loving you now.\"\n",
    "\n",
    "# 단어 토큰화\n",
    "tokenized_words=word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "01173425",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['you', 'are', 'so', 'love', '.', 'i', 'am', 'love', 'you', 'now', '.']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 포터스테머 활용\n",
    "porter_stemmer_word=[]\n",
    "for i in tokenized_words :\n",
    "    word = porter_stemmer.stem(i)\n",
    "    porter_stemmer_word.append(word)\n",
    "porter_stemmer_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "73134ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "어간추출 전 :  ['You', 'are', 'so', 'lovely', '.', 'I', 'am', 'loving', 'you', 'now', '.']\n",
      "어간추출 후 :  ['you', 'are', 'so', 'love', '.', 'i', 'am', 'love', 'you', 'now', '.']\n"
     ]
    }
   ],
   "source": [
    "# 어간추출 전/후 비교\n",
    "print('어간추출 전 : ',tokenized_words)\n",
    "print('어간추출 후 : ',porter_stemmer_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1d8312ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7cb90ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 랭커스터 스테머 객체 생성\n",
    "lancaster_stemmer = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6adb60d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#단어토큰화\n",
    "text = \"You are so lovely. I am loving you now.\"\n",
    "tokenized_words=word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b92c67c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['you', 'ar', 'so', 'lov', '.', 'i', 'am', 'lov', 'you', 'now', '.']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#단어를 돌아가면서 어간추출 진행\n",
    "lancaster_stemmed_word=[]\n",
    "for i in tokenized_words :\n",
    "    word = lancaster_stemmer.stem(i)\n",
    "    lancaster_stemmed_word.append(word)\n",
    "lancaster_stemmed_word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb14756",
   "metadata": {},
   "source": [
    "### 어간추출 함수 코드화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "146d7f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "#포터스테머 어간 추출함수\n",
    "def stemming_by_porter(tokenized_words):\n",
    "    porter_stemmer = PorterStemmer()\n",
    "    porter_stemmer_words = []\n",
    "    for i in tokenized_words :\n",
    "        word = porter_stemmer.stem(i)\n",
    "        porter_stemmer_words.append(word)    \n",
    "    return porter_stemmer_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766d01cb",
   "metadata": {},
   "source": [
    "### 문장 토큰화\n",
    " - 단어의 품사는 문장 안에서 사용되는 위치에 따라 달라질 수 있기 떄문에 문장 토근화 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b5c97e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰화 함수와 모듈 불어오기\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "15906d19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"My email address is 'abcde@smhrd.com'.\", 'Send it to Mr.Kim.']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"My email address is 'abcde@smhrd.com'. Send it to Mr.Kim.\"\n",
    "\n",
    "# 문장 토큰화\n",
    "# 약어와 같은 (Mr.,Dr.) 언어적인 특성을 고려해서 토큰화 진행\n",
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "aaf6892d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Can you forward my email to Mr.Kim?', 'Thank you!']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Can you forward my email to Mr.Kim? Thank you!\"\n",
    "\n",
    "# 느낌표, 물음표 등도 문장을 나누는 기준으로 작용\n",
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b8b39a",
   "metadata": {},
   "source": [
    "### 품사 태깅(POS : Part of Speach Taggind)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967480eb",
   "metadata": {},
   "source": [
    "- 각 단어가 어떤 품사로 쓰였는지 표시하는 작업 = 품사태깅\n",
    "- 여러문장코퍼스->문장 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b4fd2b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\SeeUSoon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "40c2a1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Watching Time Chasers, it obvious that it was made by a bunch of friends. Maybe they were sitting around one day in film school and said, \\\"Hey, let\\'s pool our money together and make a really bad movie!\\\" Or something like that.\"\n",
    "\n",
    "tokenized_sents = sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6f411d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 품사태깅에 필요한 도구 불러오기\n",
    "from nltk.tag import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d50f6af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tagged_list = []\n",
    "\n",
    "for sentence in tokenized_sents:\n",
    "    # 단어 토큰화\n",
    "    tokenized_words = word_tokenize(sentence)\n",
    "    # 품사 태깅\n",
    "    pos_tagged = pos_tag(tokenized_words)    \n",
    "    # pos_tagged_list에 값 담아주기\n",
    "    # 리스트로 끊어진 문장의 구분 없애기 - extend활용\n",
    "    pos_tagged_list.extend(pos_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f8813d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Watching', 'VBG'), ('Time', 'NNP'), ('Chasers', 'NNPS'), (',', ','), ('it', 'PRP'), ('obvious', 'VBZ'), ('that', 'IN'), ('it', 'PRP'), ('was', 'VBD'), ('made', 'VBN'), ('by', 'IN'), ('a', 'DT'), ('bunch', 'NN'), ('of', 'IN'), ('friends', 'NNS'), ('.', '.'), ('Maybe', 'RB'), ('they', 'PRP'), ('were', 'VBD'), ('sitting', 'VBG'), ('around', 'IN'), ('one', 'CD'), ('day', 'NN'), ('in', 'IN'), ('film', 'NN'), ('school', 'NN'), ('and', 'CC'), ('said', 'VBD'), (',', ','), ('``', '``'), ('Hey', 'NNP'), (',', ','), ('let', 'VB'), (\"'s\", 'POS'), ('pool', 'VB'), ('our', 'PRP$'), ('money', 'NN'), ('together', 'RB'), ('and', 'CC'), ('make', 'VB'), ('a', 'DT'), ('really', 'RB'), ('bad', 'JJ'), ('movie', 'NN'), ('!', '.'), (\"''\", \"''\"), ('Or', 'CC'), ('something', 'NN'), ('like', 'IN'), ('that', 'DT'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "print(pos_tagged_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248d5346",
   "metadata": {},
   "source": [
    "### 품사 태깅 함수 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f43ac873",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "# 품사 태깅 함수 정의\n",
    "def pos_tagger(tokenized_sents):        \n",
    "    pos_tagged_words = []\n",
    "    for sentence in tokenized_sents:\n",
    "        # 단어 토큰화\n",
    "        tokenized_words = word_tokenize(sentence)\n",
    "        # 품사 태깅\n",
    "        pos_tagged = pos_tag(tokenized_words)    \n",
    "        # 품사태깅한 데이터 담아주기(extend)\n",
    "        pos_tagged_words.extend(pos_tagged)\n",
    "    return pos_tagged_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694b7adb",
   "metadata": {},
   "source": [
    "### 표제어 추출\n",
    "- 표제어를 기준으로 통합하면 단어 정규화가 이루어짐\n",
    "- 단어의 근본적인 의미로 통합해주기\n",
    "- am, are, is -> be\n",
    "- happier, haappiest = happy\n",
    "- 표제어 추출에는 WordNet POS Tag를사용하여 품사 태깅 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "da431146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 표제어 추출을 하기 위한 품사 태깅 진행\n",
    "text = 'You are the happiest person.'\n",
    "# 단어 토큰화 word_tokenize\n",
    "tokenized_words = word_tokenize(text)\n",
    "\n",
    "# 품사태그 pos_tag\n",
    "taggeed_words=pos_tag(tokenized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9055e155",
   "metadata": {},
   "source": [
    "### WordNet Pos Tag : 영어 어휘 데이터베이스에 적용되어 있는 품사태그\n",
    "- N으로 시작하면 wn.NOUN\n",
    "- J로 시작하면 wn.ADJ\n",
    "- R로 시작하면 wn.ADV\n",
    "- V로 시작하면 wn.VERB로 통일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e6b3567e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "978a5ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pennTreePosTag를 WordNetPosTag로 변환\n",
    "def penn_to_wn(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    elif tag.startswith('N'):\n",
    "         return wn.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "         return wn.ADV\n",
    "    elif tag.startswith('V'):\n",
    "         return wn.VERB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "627dc1c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\SeeUSoon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\SeeUSoon\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 표제어 추출을 위한 함수 호출\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4eb60702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 표제어 추출하기 위한 WordNetLemmatizer 클래스에 있는 lemmatizer()함수 호출\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_words = []\n",
    "\n",
    "for word, tag in taggeed_words:\n",
    "    # WordNet pos Tag로 변환하는 함수 호출\n",
    "    wn_tag = penn_to_wn(tag)\n",
    "    \n",
    "    # 품사를 기준으로 표제어 추출\n",
    "    if wn_tag in (wn.NOUN, wn.ADJ, wn.ADV, wn.VERB):\n",
    "        lemmatized_words.append(lemmatizer.lemmatize(word, wn_tag)) # 표제어 추출함수\n",
    "    else :\n",
    "        lemmatized_words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "de95eeea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['You', 'be', 'the', 'happy', 'person', '.']\n"
     ]
    }
   ],
   "source": [
    "# 표제어 추출 확인\n",
    "print(lemmatized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a33a8d",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3f845002",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\SeeUSoon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\SeeUSoon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# pennTreePosTag를 WordNetPosTag로 변환\n",
    "def penn_to_wn(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    elif tag.startswith('N'):\n",
    "         return wn.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "         return wn.ADV\n",
    "    elif tag.startswith('V'):\n",
    "         return wn.VERB\n",
    "    else:\n",
    "        return\n",
    "        \n",
    "def words_lemmatizer(pos_tagged_words):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = []\n",
    "\n",
    "    for word, tag in pos_tagged_words:\n",
    "        # WordNet pos Tag로 변환하는 함수 호출\n",
    "        wn_tag = penn_to_wn(tag)\n",
    "\n",
    "        # 품사를 기준으로 표제어 추출\n",
    "        if wn_tag in (wn.NOUN, wn.ADJ, wn.ADV, wn.VERB):\n",
    "            lemmatized_words.append(lemmatizer.lemmatize(word, wn_tag)) # 표제어 추출함수\n",
    "        else :\n",
    "            lemmatized_words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c466dac5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
